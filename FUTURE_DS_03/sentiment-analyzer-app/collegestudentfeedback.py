# -*- coding: utf-8 -*-
"""CollegeStudentFeedback.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bAahnPGQ8-J_cuZ4_YvhedCXzekI4AGb

# Data Science & Analytics Task 3

# College Event Feedback Analysis â€“ Internship Project

# Upload & Load CSV in Colab
"""

import pandas as pd

# Upload the CSV file
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("Student_Satisfaction_Survey(1).csv", encoding='cp1252')
df.head(10)

"""## Check Data Info"""

df.info()       # Shows column names and types
df.columns      # Lists all column names

"""# 1. Shape of the dataset"""

df.shape

"""## 2. List of columns"""

df.columns.tolist()

"""## 3. Data types of each column"""

df.dtypes

"""## 4. Info summary"""

df.info()

"""## 5. Preview the first few rows"""

df.head(10)

# @title Total Feedback Given

from matplotlib import pyplot as plt
df['Total Feedback Given'].plot(kind='line', figsize=(8, 4), title='Total Feedback Given')
plt.gca().spines[['top', 'right']].set_visible(False)

"""## 6. Preview the last few rows"""

df.tail(10)

"""## Check for missing values


"""

df.isnull().sum()

"""## 8. Summary statistics"""

df.describe()

"""## 9. Check for duplicates"""

df.duplicated().sum()

"""## 10. Check unique values in a column"""

df['Questions'].unique()

"""## 11. Count of unique values per column"""

df.nunique()

"""## Describe everything summary(One line summary)"""

df.describe(include='all')

print(df['Average/ Percentage'].unique())

"""### Check for the trailing space"""

print(df.columns.tolist())

"""## Removing  the trailing space"""

df.columns = df.columns.str.strip()

print(df.columns.tolist())

"""## Converting Average/Percentage columns to string to numeric

### Parse the Average/ Percentage column
Right now it looks like "3.00 / 60.00" as a string. We want to split it into two numeric columns:

Average_Score = 3.00 (float)

Percentage = 60.00 (float)
"""

# Split the string by '/'
df[['Average_Score', 'Percentage']] = df['Average/ Percentage'].str.split('/', expand=True)

# Remove whitespace and convert to float
df['Average_Score'] = df['Average_Score'].str.strip().astype(float)
df['Percentage'] = df['Percentage'].str.strip().astype(float)

df.describe(include='all')

df.info()

df.dtypes

# Dropping  original column as it's no longer needed
df.drop(columns=['Average/ Percentage'], inplace=True)

df.info()

"""### Check the Weightage columns"""

weightage_cols = ['Weightage 1', 'Weightage 2', 'Weightage 3', 'Weightage 4', 'Weightage 5']
print(df[weightage_cols].dtypes)

"""## Drop SN columns because its not useful"""

df.drop(columns=['SN'], inplace=True)

df.dtypes

"""## Chcek for missing Values"""

print(df.isnull().sum())

# Check how many duplicate rows exist
duplicate_count = df.duplicated().sum()
print(f"Total duplicate rows: {duplicate_count}")

duplicates = df[df.duplicated()]
print(duplicates)

df.duplicated(subset=['Course Name', 'Questions', 'Average_Score'])

"""## Dataset is Cleaned and there is no duplicate values

# EDA(Exploratory Data Analaysis)

#### Understand the  dataset using statistics and visualize patterns in:

Ratings (Average_Score, Percentage)

Feedback question types

Course-wise feedback

## Summary Statistics
"""

df.describe()

df.head(10)

"""## Visualize Distribution of Average Scores"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.histplot(df['Average_Score'], bins=5, kde=True, color='blue')
plt.title('Distribution of Average Scores')
plt.xlabel('Average Score')
plt.ylabel('Number of Responses')
plt.grid(False)
plt.show()

"""## Average Score per Course"""

plt.figure(figsize=(12,6))
avg_by_course = df.groupby('Course Name')['Average_Score'].mean().sort_values(ascending=False)

sns.barplot(x=avg_by_course.index, y=avg_by_course.values, palette='viridis')
plt.xticks(rotation=90)
plt.title('Average Score by Course')
plt.ylabel('Average Score')
plt.xlabel('Course Name')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Count of Each Rating (1 to 5)"""

weightage_cols = ['Weightage 1', 'Weightage 2', 'Weightage 3', 'Weightage 4', 'Weightage 5']
rating_counts = df[weightage_cols].sum()

plt.figure(figsize=(8,5))
sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='coolwarm')
plt.title('Total Responses per Rating (1 to 5)')
plt.xlabel('Rating')
plt.ylabel('Total Responses')
plt.grid(False)
plt.show()

"""## Boxplot to See Spread of Scores"""

plt.figure(figsize=(8,5))
sns.boxplot(data=df, x='Average_Score', color='lightgreen')
plt.title('Boxplot of Average Scores')
plt.grid(False)
plt.show()

"""## Pie Chart of Total Responses per Rating (1â€“5)"""

rating_counts = df[['Weightage 1', 'Weightage 2', 'Weightage 3', 'Weightage 4', 'Weightage 5']].sum()
labels = [col.split()[-1] for col in rating_counts.index]  # Extract just the rating number

plt.figure(figsize=(7,7))
plt.pie(rating_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
plt.title('Rating Distribution (Pie Chart)')
plt.axis('equal')
plt.show()

"""## Line Chart of Average Score Trends per Course"""

avg_by_course_sorted = df.groupby('Course Name')['Average_Score'].mean().sort_values()

plt.figure(figsize=(12,6))
sns.lineplot(x=avg_by_course_sorted.index, y=avg_by_course_sorted.values, marker='o', color='orange')
plt.xticks(rotation=90)
plt.title('Average Score Trend by Course')
plt.xlabel('Course Name')
plt.ylabel('Average Score')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Average Score trend by Percentage"""

avg_by_Percentage_sorted = df.groupby('Course Name')['Percentage'].mean().sort_values()

plt.figure(figsize=(12,6))
sns.lineplot(x=avg_by_Percentage_sorted.index, y=avg_by_Percentage_sorted.values, marker='o', color='blue')
plt.xticks(rotation=90)
plt.title('Average percentage Trend by Course')
plt.xlabel('Course Name')
plt.ylabel('Percentage')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Count of Feedback Entries per Course (Bar Chart)"""

plt.figure(figsize=(12,6))
course_counts = df['Course Name'].value_counts()

sns.barplot(x=course_counts.index, y=course_counts.values, palette='mako')
plt.xticks(rotation=90)
plt.title('Number of Feedback Entries per Course')
plt.xlabel('Course Name')
plt.ylabel('Number of Entries')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Average Percentage per Course"""

plt.figure(figsize=(12,6))
avg_percentage = df.groupby('Course Name')['Percentage'].mean().sort_values(ascending=False)

sns.barplot(x=avg_percentage.index, y=avg_percentage.values, palette='cubehelix')
plt.xticks(rotation=90)
plt.title('Average Percentage Score by Course')
plt.ylabel('Percentage (%)')
plt.xlabel('Course Name')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Sum of Average_Score by Questions"""

plt.figure(figsize=(14,6))
avg_score_sum = df.groupby('Questions')['Average_Score'].sum().sort_values(ascending=False)

sns.barplot(x=avg_score_sum.values, y=avg_score_sum.index, palette='crest')
plt.title('Sum of Average Scores by Question')
plt.xlabel('Sum of Average Score')
plt.ylabel('Feedback Question')
plt.grid(False)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Group by question
grouped = df.groupby('Questions')['Average_Score'].agg(['sum', 'mean']).sort_values(by='sum', ascending=False)

# Plot the sum
plt.figure(figsize=(14,6))
barplot = sns.barplot(x=grouped['sum'], y=grouped.index, palette='crest')

# Add mean value as text label on each bar
for i, (value, mean_val) in enumerate(zip(grouped['sum'], grouped['mean'])):
    plt.text(value + 0.5, i, f'Avg: {mean_val:.2f}', va='center', fontsize=9)

plt.title('Sum of Average Scores by Question (with Avg Labels)')
plt.xlabel('Sum of Average Score')
plt.ylabel('Feedback Question')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Sum of Percentage by Questions"""

plt.figure(figsize=(14,6))
percentage_sum = df.groupby('Questions')['Percentage'].sum().sort_values(ascending=False)

sns.barplot(x=percentage_sum.values, y=percentage_sum.index, palette='flare')
plt.title('Sum of Percentage Scores by Question')
plt.xlabel('Sum of Percentage')
plt.ylabel('Feedback Question')
plt.grid(False)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Group by Questions and sum Percentage
percentage_sum = df.groupby('Questions')['Percentage'].sum().sort_values(ascending=False)

plt.figure(figsize=(14,6))
barplot = sns.barplot(x=percentage_sum.values, y=percentage_sum.index, palette='flare')

# Add sum values as text labels at the end of each bar
for i, val in enumerate(percentage_sum.values):
    plt.text(val + 0.5, i, f'{val:.2f}', va='center', fontsize=9)

plt.title('Sum of Percentage Scores by Question')
plt.xlabel('Sum of Percentage')
plt.ylabel('Feedback Question')
plt.grid(False)
plt.tight_layout()
plt.show()

"""## Genrate random 580comments"""

import pandas as pd
import random
from textblob import TextBlob

# Set seed for reproducibility
random.seed(42)

# Total
n = 580
pos_n = int(0.65 * n)
neu_n = int(0.20 * n)
neg_n = n - pos_n - neu_n

# Comment banks
positive_comments = [
    "The session was fantastic and very engaging.",
    "Excellent speakers and content!",
    "I enjoyed the activities a lot!",
    "Loved every bit of it!",
    "Highly satisfied with the course.",
    "Teaching was outstanding.",
    "Great delivery by the instructor.",
    "The content was very informative.",
    "Mentoring helped me improve a lot.",
    "Interactive and enjoyable sessions."
]

neutral_comments = [
    "It was okay, but a bit long.",
    "Could be improved with more examples.",
    "It was average.",
    "The explanation was just satisfactory.",
    "The session was neither good nor bad.",
    "Some parts were useful.",
    "Content was fine.",
    "The delivery was acceptable.",
    "Neutral experience overall.",
    "No comments."
]

negative_comments = [
    "Not what I expected, felt boring.",
    "Too theoretical for my taste.",
    "The pace was too fast.",
    "I couldn't understand most of it.",
    "The teacher didn't seem well-prepared.",
    "Found the session confusing.",
    "Not satisfied with the teaching method.",
    "Examples were not clear.",
    "Low interaction in the class.",
    "Assignments were not discussed properly."
]

# Assign comments
comments = (
    random.choices(positive_comments, k=pos_n) +
    random.choices(neutral_comments, k=neu_n) +
    random.choices(negative_comments, k=neg_n)
)
random.shuffle(comments)

# Create DataFrame
df = pd.DataFrame({"Comments": comments})

# Add polarity
df["Polarity"] = df["Comments"].apply(lambda x: TextBlob(x).sentiment.polarity)

# Label sentiment
def label_sentiment(p):
    if p > 0.2:
        return "Positive"
    elif p < -0.1:
        return "Negative"
    else:
        return "Neutral"

df["Sentiment"] = df["Polarity"].apply(label_sentiment)

# View sample
print(df.head())

df.dtypes

print(df)

df.info()       # Shows column names and types
df.columns      # Lists all column names

df.columns.tolist()

df.shape

"""# Adding New Dataset to Old Dataset"""

import pandas as pd

# Upload the CSV file
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("Student_Satisfaction_Survey(1).csv", encoding='cp1252')
df.head(10)

import random
from textblob import TextBlob

random.seed(42)
n = len(df)  # Use length of your original df to match rows
pos_n = int(0.65 * n)
neu_n = int(0.20 * n)
neg_n = n - pos_n - neu_n

positive_comments = [
    "The session was fantastic and very engaging.",
    "Excellent speakers and content!",
    "I enjoyed the activities a lot!",
    "Loved every bit of it!",
    "Highly satisfied with the course.",
    "Teaching was outstanding.",
    "Great delivery by the instructor.",
    "The content was very informative.",
    "Mentoring helped me improve a lot.",
    "Interactive and enjoyable sessions."
]

neutral_comments = [
    "It was okay, but a bit long.",
    "Could be improved with more examples.",
    "It was average.",
    "The explanation was just satisfactory.",
    "The session was neither good nor bad.",
    "Some parts were useful.",
    "Content was fine.",
    "The delivery was acceptable.",
    "Neutral experience overall.",
    "No comments."
]

negative_comments = [
    "Not what I expected, felt boring.",
    "Too theoretical for my taste.",
    "The pace was too fast.",
    "I couldn't understand most of it.",
    "The teacher didn't seem well-prepared.",
    "Found the session confusing.",
    "Not satisfied with the teaching method.",
    "Examples were not clear.",
    "Low interaction in the class.",
    "Assignments were not discussed properly."
]

# Generate comments list
comments = (
    random.choices(positive_comments, k=pos_n) +
    random.choices(neutral_comments, k=neu_n) +
    random.choices(negative_comments, k=neg_n)
)
random.shuffle(comments)

# Add comments to existing DataFrame
df["Comments"] = comments

# Add polarity column
df["Polarity"] = df["Comments"].apply(lambda x: TextBlob(x).sentiment.polarity)

# Sentiment labeling function
def label_sentiment(p):
    if p > 0.2:
        return "Positive"
    elif p < -0.1:
        return "Negative"
    else:
        return "Neutral"

df["Sentiment"] = df["Polarity"].apply(label_sentiment)

# Check your DataFrame
print(df.columns)  # Should show original + Comments + Polarity + Sentiment
print(df.head())

df.shape

print(df.isnull().sum())

# Check how many duplicate rows exist
duplicate_count = df.duplicated().sum()
print(f"Total duplicate rows: {duplicate_count}")

df.columns = df.columns.str.strip()
print(df.columns.tolist())

# Split the string by '/'
df[['Average_Score', 'Percentage']] = df['Average/ Percentage'].str.split('/', expand=True)

# Remove whitespace and convert to float
df['Average_Score'] = df['Average_Score'].str.strip().astype(float)
df['Percentage'] = df['Percentage'].str.strip().astype(float)

df.dtypes

"""## Explore & Visualize Sentiment Distribution


"""

import matplotlib.pyplot as plt

sentiment_counts = df["Sentiment"].value_counts(normalize=True) * 100
print(sentiment_counts)

sentiment_counts.plot(kind='bar', color=['green','gray','red'])
plt.title("Sentiment Distribution (%)")
plt.ylabel("Percentage")
plt.show()

"""## Sum of average ratings and sum of percentage by comments"""

import matplotlib.pyplot as plt

# Group by Comments (the actual text)
avg_score_by_comment = df.groupby('Comments')['Average_Score'].mean()
avg_percentage_by_comment = df.groupby('Comments')['Percentage'].mean()

print("Average Score by Comments:\n", avg_score_by_comment.head())
print("\nAverage Percentage by Comments:\n", avg_percentage_by_comment.head())

# Optional: plot top 10 comments by average score (sorted descending)
top_comments = avg_score_by_comment.sort_values(ascending=False).head(10)

plt.figure(figsize=(10,6))
top_comments.plot(kind='bar')
plt.title("Top 10 Comments by Average Score")
plt.ylabel("Average Score")
plt.xlabel("Comment")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

print(df.columns)

import matplotlib.pyplot as plt

# Group by Comments
avg_score_by_comment = df.groupby('Comments')['Total Feedback Given'].sum()  # or use sum() if you want sum
sum_percentage_by_comment = df.groupby('Comments')['Percentage'].sum()

# For better visualization, pick top 10 comments by average score
top_comments = avg_score_by_comment.sort_values(ascending=False).head(15)

# Filter sum_percentage accordingly
top_percentage = sum_percentage_by_comment.loc[top_comments.index]

fig, ax1 = plt.subplots(figsize=(12, 6))

# Bar chart for Average Score
bars = ax1.bar(top_comments.index, top_comments.values, color='skyblue', label='Average Score')
ax1.set_xlabel('Comments')
ax1.set_ylabel('Average_Score', color='skyblue')
ax1.tick_params(axis='y', labelcolor='skyblue')
ax1.set_xticklabels(top_comments.index, rotation=45, ha='right')

# Create a secondary axis for Percentage
ax2 = ax1.twinx()
line = ax2.plot(top_percentage.index, top_percentage.values, color='orange', marker='o', label='Sum of Percentage')
ax2.set_ylabel('Sum of Percentage', color='orange')
ax2.tick_params(axis='y', labelcolor='orange')

# Title and legend
plt.title('Average Score (Bar) and Sum of Percentage (Line) by Comments')

# Combine legends
lines_labels = [bars, line[0]]
labels = [l.get_label() for l in lines_labels]
ax1.legend(lines_labels, labels, loc='upper left')

plt.tight_layout()
plt.show()

df.dtypes

"""## Sum Of Percentage by Comments"""

import matplotlib.pyplot as plt

# Group by Comments and calculate sum of Percentage
sum_percentage = df.groupby('Comments')['Percentage'].sum()

# Optional: Display top 10 comments for better clarity
top_comments = sum_percentage.sort_values(ascending=False).head(10)

# Plot
plt.figure(figsize=(12, 6))
bars = plt.bar(top_comments.index, top_comments.values, color='mediumseagreen')

plt.xlabel('Comments')
plt.ylabel('Sum of Percentage')
plt.title('Sum of Percentage by Comments')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

import matplotlib.pyplot as plt

# Group and take top 10 for readability
sum_percentage = df.groupby('Comments')['Percentage'].sum()
top_comments = sum_percentage.sort_values(ascending=False).head(10)

# Plot pie chart
plt.figure(figsize=(8, 8))
plt.pie(top_comments.values, labels=top_comments.index, autopct='%1.1f%%', startangle=140)
plt.title('Percentage Distribution by Comments')
plt.tight_layout()
plt.show()



"""## Sum Of Percentage by Comments"""

import matplotlib.pyplot as plt

# Group and take top 10
sum_percentage = df.groupby('Comments')['Percentage'].sum()
top_comments = sum_percentage.sort_values(ascending=True).head(10)

# Horizontal bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_comments.index, top_comments.values, color='coral')
plt.xlabel('Sum of Percentage')
plt.title('Sum of Percentage by Comments')
plt.tight_layout()
plt.show()

"""# NLP Process on Feedback Comments
 Total Comments: 580
 Goal: Analyze feedback comments using NLP to extract insights like sentiment, keyword importance, frequent topics

# Text Preprocessing
"""

pip install nltk

import nltk
nltk.download('punkt')

import nltk
nltk.data.path.append("C:/Users/Gouthum/nltk_data")

"""## Cleaned Comments"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure required resources are downloaded
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize lemmatizer and stopwords
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Custom tokenizer (avoid using word_tokenize to bypass punkt issue)
def simple_tokenizer(text):
    # Remove non-alphabetic characters and split
    return re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())

# Clean text function without punkt
def clean_text(text):
    tokens = simple_tokenizer(text)
    filtered = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(filtered)

# Apply to your DataFrame
df['Cleaned_Comments'] = df['Comments'].apply(clean_text)

# View result
df[['Comments', 'Cleaned_Comments']].head()

"""## Sentiment Analysis with TextBlob"""

from textblob import TextBlob

df["Polarity"] = df["Comments"].apply(lambda x: TextBlob(x).sentiment.polarity)
df["Subjectivity"] = df["Comments"].apply(lambda x: TextBlob(x).sentiment.subjectivity)

def get_sentiment(p):
    if p > 0.1:
        return "Positive"
    elif p < -0.1:
        return "Negative"
    else:
        return "Neutral"

df["Sentiment"] = df["Polarity"].apply(get_sentiment)

df[['Comments', 'Polarity', 'Subjectivity', 'Sentiment']].head(15)
df[['Comments', 'Polarity', 'Subjectivity', 'Sentiment']].head(10)

from matplotlib import pyplot as plt
import seaborn as sns
_df_2.groupby('Sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""# 1ï¸âƒ£ Sum of Average Score by Sentiment"""

import matplotlib.pyplot as plt

# Prepare the data
avg_sum = df.groupby("Sentiment")["Average_Score"].sum().reset_index()

# Define custom colors for each sentiment
sentiment_colors = {
    "Positive": "green",
    "Neutral": "gold",
    "Negative": "red"
}
colors = avg_sum["Sentiment"].map(sentiment_colors)

# Create the plot
plt.figure(figsize=(8, 5))
bars = plt.bar(avg_sum["Sentiment"], avg_sum["Average_Score"], color=colors)

# Add value labels on top of bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f'{yval:.1f}', ha='center', va='bottom', fontsize=11)

# Titles and labels
plt.title("Sum of Average Score by Sentiment", fontsize=14)
plt.ylabel("Total Average Score", fontsize=12)

# Remove gridlines
plt.grid(False)

# Remove top and right border
plt.box(False)

plt.tight_layout()
plt.show()

"""# 2ï¸âƒ£ Sum of Percentage Score by Sentiment


"""

# Prepare the data
perc_sum = df.groupby("Sentiment")["Percentage"].sum().reset_index()

# Define custom colors
sentiment_colors = {
    "Positive": "green",
    "Neutral": "gold",
    "Negative": "red"
}
colors = perc_sum["Sentiment"].map(sentiment_colors)

# Plot
plt.figure(figsize=(8, 5))
bars = plt.bar(perc_sum["Sentiment"], perc_sum["Percentage"], color=colors)

# Add value labels
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f'{yval:.1f}', ha='center', va='bottom', fontsize=11)

plt.title("Sum of Percentage Score by Sentiment", fontsize=14)
plt.ylabel("Total Percentage", fontsize=12)
plt.grid(False)
plt.box(False)
plt.tight_layout()
plt.show()

"""#3ï¸âƒ£ Sum of Average Score and Percentage by Sentiment (Grouped Bar Chart)


"""

import numpy as np

# Grouped data
grouped = df.groupby("Sentiment")[["Average_Score", "Percentage"]].sum().reset_index()

# Plotting
labels = grouped["Sentiment"]
x = np.arange(len(labels))
width = 0.35

plt.figure(figsize=(9, 5))
bars1 = plt.bar(x - width/2, grouped["Average_Score"], width, label='Average Score', color='skyblue')
bars2 = plt.bar(x + width/2, grouped["Percentage"], width, label='Percentage', color='orange')

# Value labels
for i in range(len(x)):
    plt.text(x[i] - width/2, grouped["Average_Score"][i] + 1, f'{grouped["Average_Score"][i]:.1f}', ha='center', fontsize=10)
    plt.text(x[i] + width/2, grouped["Percentage"][i] + 1, f'{grouped["Percentage"][i]:.1f}', ha='center', fontsize=10)

# Styling
plt.xticks(x, labels)
plt.title("Sum of Average Score and Percentage by Sentiment", fontsize=14)
plt.ylabel("Score", fontsize=12)
plt.legend()
plt.grid(False)
plt.box(False)
plt.tight_layout()
plt.show()

"""# Sum Of percentage share By sentiment"""

import matplotlib.pyplot as plt

# Group and calculate percentage distribution
percent_grouped = df.groupby("Sentiment")["Percentage"].sum()
total_percentage = percent_grouped.sum()
percentage_share = (percent_grouped / total_percentage) * 100

# Colors
colors = ['green', 'gold', 'red']

# Plot
plt.figure(figsize=(6, 6))
plt.pie(
    percentage_share,
    labels=[f"{label} ({val:.1f}%)" for label, val in zip(percentage_share.index, percentage_share)],
    colors=colors,
    startangle=140,
    autopct='%1.1f%%',
    textprops={'fontsize': 12}
)
plt.title("Percentage Share by Sentiment", fontsize=14)
plt.axis('equal')  # Equal aspect ratio for circle
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Grouping your data
grouped = df.groupby("Sentiment")[["Average_Score", "Percentage"]].sum()
grouped = grouped.loc[["Positive", "Neutral", "Negative"]]  # Ensure consistent order

# Setup
x = np.arange(len(grouped))
fig, ax1 = plt.subplots(figsize=(8, 5))

# Bar chart on primary y-axis
bar = ax1.bar(x, grouped["Average_Score"], width=0.4, color=['green', 'gold', 'red'], label="Average Score")
ax1.set_ylabel("Sum of Average Score", color='black')
ax1.tick_params(axis='y', labelcolor='black')


for b in bar:
    height = b.get_height()
    ax1.text(b.get_x() + b.get_width()/2, height + 8, f"{height:.1f}", ha='center', fontsize=14, color='black')

# Line chart on secondary y-axis
ax2 = ax1.twinx()
ax2.plot(x, grouped["Percentage"], color='blue', marker='o', label="Sum of Percentage")
ax2.set_ylabel("Sum of Percentage (%)", color='blue')
ax2.tick_params(axis='y', labelcolor='blue')

# Line chart value labels
for i, val in enumerate(grouped["Percentage"]):
    ax2.text(x[i], val +1, f"{val:.1f}%", ha='center', color='blue', fontsize=7)

# X-axis setup
ax1.set_xticks(x)
ax1.set_xticklabels(grouped.index)
ax1.set_title("Sum of Average Score and Percentage by Sentiment")

# Clean up
ax1.grid(False)
ax2.grid(False)

plt.tight_layout()
plt.show()

import numpy as np

# Bar chart setup
x = np.arange(len(grouped))
width = 0.35

fig, ax = plt.subplots(figsize=(8, 5))
bar1 = ax.bar(x - width/2, grouped["Average_Score"], width, label="Average Score", color='skyblue')
bar2 = ax.bar(x + width/2, grouped["Percentage"], width, label="Percentage", color='orange')

# Labels
ax.set_xticks(x)
ax.set_xticklabels(grouped.index)
ax.set_ylabel("Values")
ax.set_title("Sum of Average Score and Percentage by Sentiment")
ax.legend()

# Remove grid
ax.grid(False)

# Add bar labels
for bars in [bar1, bar2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, height + 1, f"{height:.1f}", ha='center')

plt.tight_layout()
plt.show()

"""## Count of questions per sentiment"""

import matplotlib.pyplot as plt

# Count of questions by Sentiment
sentiment_counts = df['Sentiment'].value_counts().reindex(['Positive', 'Neutral', 'Negative'])

# Plot bar chart
plt.figure(figsize=(5, 5))
colors = ['green', 'yellow', 'red']
bars = plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors)

# Add value labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 2, str(height), ha='center', fontsize=11)

plt.title('Count of Questions by Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Number of Questions')
plt.ylim(0, max(sentiment_counts.values)*1.2)
plt.grid(False)
plt.show()

import matplotlib.pyplot as plt

sentiment_counts = df['Sentiment'].value_counts().reindex(['Positive', 'Neutral', 'Negative'])
colors = ['green', 'yellow', 'red']

plt.figure(figsize=(4,4))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors, startangle=140, textprops={'fontsize': 12})
plt.title('Distribution of Questions by Sentiment', fontsize=14)
plt.axis('equal')  # Equal aspect ratio ensures pie is a circle
plt.show()

plt.figure(figsize=(4,4))
wedges, texts, autotexts = plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors, startangle=140, pctdistance=0.85, textprops={'fontsize': 12})

# Draw center circle
centre_circle = plt.Circle((0,0),0.70,fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Distribution of Questions by Sentiment (Donut Chart)', fontsize=14)
plt.axis('equal')
plt.show()

# Plot stacked bar chart
plt.figure(figsize=(6, 6))
sentiment_counts.plot(kind='bar', stacked=True,
                      color=['green', 'yellow', 'red'])  # Positive, Neutral, Negative colors

plt.title('Sentiment Distribution by Questions')
plt.xlabel('Questions')
plt.ylabel('Count of Comments')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Sentiment')
plt.grid(False)  # No gridlines
plt.tight_layout()
plt.show()

"""#Generate  wordcloud per sentiment"""

pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Separate comments by sentiment
positive_text = " ".join(df[df['Sentiment'] == 'Positive']['Cleaned_Comments'])
neutral_text = " ".join(df[df['Sentiment'] == 'Neutral']['Cleaned_Comments'])
negative_text = " ".join(df[df['Sentiment'] == 'Negative']['Cleaned_Comments'])

# Function to plot word cloud
def plot_wordcloud(text, title, color):
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                          colormap=color, max_words=100).generate(text)
    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

# Plot for each sentiment
plot_wordcloud(positive_text, "Positive Comments Word Cloud", 'Greens')
plot_wordcloud(neutral_text, "Neutral Comments Word Cloud", 'Oranges')
plot_wordcloud(negative_text, "Negative Comments Word Cloud", 'Reds')

"""# Common word in comments(wordcloud)"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all cleaned comments into one string
all_comments_text = " ".join(df['Cleaned_Comments'])

# Generate and plot the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=150).generate(all_comments_text)

plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of All Comments", fontsize=18)
plt.show()

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

def vader_sentiment(text):
    score = analyzer.polarity_scores(text)['compound']
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['Vader_Sentiment'] = df['Comments'].apply(vader_sentiment)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine cleaned comments by Vader sentiment
positive_text = " ".join(df[df['Vader_Sentiment'] == 'Positive']['Cleaned_Comments'])
neutral_text = " ".join(df[df['Vader_Sentiment'] == 'Neutral']['Cleaned_Comments'])
negative_text = " ".join(df[df['Vader_Sentiment'] == 'Negative']['Cleaned_Comments'])

def plot_wordcloud(text, title, color):
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                          colormap=color, max_words=100).generate(text)
    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

plot_wordcloud(positive_text, "Vader Positive Comments Word Cloud", 'Greens')
plot_wordcloud(neutral_text, "Vader Neutral Comments Word Cloud", 'Oranges')
plot_wordcloud(negative_text, "Vader Negative Comments Word Cloud", 'Reds')

from wordcloud import WordCloud
import matplotlib.pyplot as plt

text = " ".join(df['Cleaned_Comments'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import matplotlib.pyplot as plt
from collections import Counter
import pandas as pd

def get_top_n_words(text_series, n=10):
    all_words = " ".join(text_series).split()
    counter = Counter(all_words)
    return counter.most_common(n)

# Extract top words per sentiment
top_n = 10
sentiments = df['Sentiment'].unique()

fig, axs = plt.subplots(1, len(sentiments), figsize=(5 * len(sentiments), 5), sharey=True)

if len(sentiments) == 1:
    axs = [axs]

for i, sentiment in enumerate(sentiments):
    subset = df[df['Sentiment'] == sentiment]['Cleaned_Comments']
    top_words = get_top_n_words(subset, top_n)
    words, counts = zip(*top_words)

    axs[i].barh(words[::-1], counts[::-1], color={
        "Positive": "green",
        "Neutral": "orange",
        "Negative": "red"
    }.get(sentiment, "gray"))

    axs[i].set_title(f"Top {top_n} Words\n({sentiment})")
    axs[i].set_xlabel("Frequency")
    axs[i].invert_yaxis()

plt.tight_layout()
plt.show()

"""# Polarity adn subjectivity column from textblob"""

import matplotlib.pyplot as plt

# Scatter plot setup
plt.figure(figsize=(8,6))

# Plot with different colors for sentiment classes
colors = {'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'}

for sentiment, color in colors.items():
    subset = df[df['Sentiment'] == sentiment]
    plt.scatter(subset['Polarity'], subset['Subjectivity'],
                color=color, label=sentiment, alpha=0.6, edgecolors='w', s=80)

plt.title("Polarity vs Subjectivity of Comments")
plt.xlabel("Polarity (-1 negative to +1 positive)")
plt.ylabel("Subjectivity (0 objective to 1 subjective)")
plt.legend(title="Sentiment")
plt.grid(False)
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import pandas as pd

def plot_top_bigrams(texts, title, color, n=10):
    # Vectorize bigrams
    vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')
    X = vectorizer.fit_transform(texts)

    # Sum up counts of each bigram
    sums = X.sum(axis=0)
    bigram_freq = [(bigram, sums[0, idx]) for bigram, idx in vectorizer.vocabulary_.items()]
    bigram_freq = sorted(bigram_freq, key=lambda x: x[1], reverse=True)[:n]

    # Convert to DataFrame for plotting
    df_bigrams = pd.DataFrame(bigram_freq, columns=['Bigram', 'Frequency'])

    # Plot bar chart
    plt.figure(figsize=(5, 5))
    plt.barh(df_bigrams['Bigram'][::-1], df_bigrams['Frequency'][::-1], color=color)
    plt.title(title)
    plt.xlabel('Frequency')
    plt.ylabel('Bigram')
    plt.show()

# Extract cleaned comments by sentiment
positive_texts = df[df['Sentiment'] == 'Positive']['Cleaned_Comments']
neutral_texts = df[df['Sentiment'] == 'Neutral']['Cleaned_Comments']
negative_texts = df[df['Sentiment'] == 'Negative']['Cleaned_Comments']

# Plot top bigrams for each sentiment
plot_top_bigrams(positive_texts, "Top 10 Bigrams in Positive Comments", 'green')
plot_top_bigrams(neutral_texts, "Top 10 Bigrams in Neutral Comments", 'orange')
plot_top_bigrams(negative_texts, "Top 10 Bigrams in Negative Comments", 'red')

"""## Sentiment analysis with VADER"""

import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# Download required VADER lexicon
nltk.download('vader_lexicon')

# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Apply sentiment analysis
df['VADER_Scores'] = df['Comments'].apply(lambda x: sia.polarity_scores(x))

# Extract compound score
df['Compound'] = df['VADER_Scores'].apply(lambda x: x['compound'])

# Label sentiment based on compound score
def vader_sentiment_label(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['VADER_Sentiment'] = df['Compound'].apply(vader_sentiment_label)

# View sample output
df[['Comments', 'Compound', 'VADER_Sentiment']].head(15)

from matplotlib import pyplot as plt
import seaborn as sns
_df_9.groupby('VADER_Sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""### I Dont Event_Type column so I will create new Event_Type column for Coorelation between rating and event type"""

df['Event_Type'] = df['Course Name'].apply(lambda x: 'Workshop' if 'workshop' in x.lower() else 'Seminar')

# Group by event type and calculate average rating
avg_rating_by_event_type = df.groupby('Event_Type')['Average_Score'].mean().sort_values(ascending=False)



"""## 1) COrealation Between Ratings and Event Type"""

print("Average Rating by Event Type:")
print(avg_rating_by_event_type)

# Optional: Calculate correlation coefficient between Average_Score and some numeric encoding of Event_Type
# First encode 'Event_Type' to numeric (e.g., Workshop=1, Seminar=0)
df['Event_Type_Num'] = df['Event_Type'].map({'Workshop': 1, 'Seminar': 0})

# Calculate correlation
correlation = df[['Average_Score', 'Event_Type_Num']].corr().loc['Average_Score', 'Event_Type_Num']

print(f"\nCorrelation between Average Score and Event Type (Workshop=1, Seminar=0): {correlation:.3f}")

import matplotlib.pyplot as plt

# Calculate average rating by event type
avg_rating_by_event = df.groupby('Event_Type')['Average_Score'].mean()
print(avg_rating_by_event)

# Plot bar chart
avg_rating_by_event.plot(kind='bar', color=['skyblue', 'orange'], figsize=(6,4))

plt.title('Average Rating by Event Type')
plt.ylabel('Average Rating')
plt.xlabel('Event Type')
plt.ylim(0, 5)
plt.xticks(rotation=0)
plt.grid(False)
plt.show()

df.dtypes

df.head(50)

df.dtypes

list(df.columns)

"""## Word cloud for the most common Complaints"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Filter only negative comments
negative_comments = df[df["Sentiment"] == "Negative"]["Cleaned_Comments"]

# Join all negative comment words into one string
neg_text = " ".join(negative_comments)

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white',
                      colormap='Reds', max_words=100).generate(neg_text)

# Plot
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Most Common Complaints (Negative Comments)", fontsize=16)
plt.show()

"""## List of Most complaints (Negative Comments)"""

from collections import Counter

# Filter negative comments
negative_comments = df[df["Sentiment"] == "Negative"]["Cleaned_Comments"]

# Split all words into a list
all_words = " ".join(negative_comments).split()

# Count word frequency
word_freq = Counter(all_words)

# Display top 15 most common complaint words
top_complaints = word_freq.most_common(15)

# Print as bullet list
print("ðŸ”´ Most Common Complaints:")
for word, freq in top_complaints:
    print(f"- {word} ({freq} times)")

"""## Rating Vs Event Type(Workshop Vs Seminar)"""

import matplotlib.pyplot as plt
import seaborn as sns

# Group by event type and calculate average score
event_rating = df.groupby("Basic Course")["Average_Score"].mean().sort_values(ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x=event_rating.values, y=event_rating.index, palette="viridis")

plt.title("Average Rating by Event Type")
plt.xlabel("Average Rating")
plt.ylabel("Event Type")
plt.grid(False)  # Remove gridlines
plt.tight_layout()
plt.show()

event_rating = (
    df.groupby("Basic Course")["Average_Score"]
    .mean()
    .sort_values(ascending=False)
)

print("ðŸ”¹ Average Rating by Event Type:\n")
for event, rating in event_rating.items():
    print(f"- {event}: {rating:.2f}")

"""## Most Liked DEpartments"""

import matplotlib.pyplot as plt
import seaborn as sns

# Group by Basic Course (department) and calculate average satisfaction score
dept_avg = df.groupby("Basic Course")["Average_Score"].mean().sort_values(ascending=False)

# Display the top departments as a list
print("ðŸ”¹ Average Rating by Department (Basic Course):\n")
for dept, rating in dept_avg.items():
    print(f"- {dept}: {rating:.2f}")

# Plotting
plt.figure(figsize=(10, 6))
sns.barplot(x=dept_avg.values, y=dept_avg.index, palette="Greens_r")

plt.title("Most-Liked Departments (Based on Avg Satisfaction)", fontsize=14)
plt.xlabel("Average Score")
plt.ylabel("Department")
plt.grid(False)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Group by department (Basic Course) and calculate average rating
dept_avg_rating = df.groupby("Basic Course")["Average_Score"].mean().sort_values(ascending=False)

# Display top 10 departments by average rating
top_departments = dept_avg_rating.head(10)
print("Top 10 Departments by Average Rating:")
print(top_departments)

# Plotting
plt.figure(figsize=(8, 6))
bars = plt.bar(top_departments.index, top_departments.values, color='mediumseagreen')

# Add value labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.03, f'{height:.2f}', ha='center', fontsize=10)

plt.title("Top 10 Departments Hosting Most-Liked Events (Average Score)")
plt.ylabel("Average Score")
plt.xticks(rotation=45, ha='right')
plt.ylim(0, max(top_departments.values) + 0.5)
plt.grid(False)
plt.tight_layout()
plt.show()

"""## ðŸ“‹ Insights & Interpretation Summary
Based on the analysis of 580 student feedback responses, a significant majority expressed positive sentiment toward the events. Students particularly appreciated the interactive sessions, engaging activities, and quality of instruction. Comments like "The session was fantastic and very engaging" and "I enjoyed the activities a lot!" were common, indicating a strong preference for sessions that included real-world examples and dynamic presentation styles. These were reflected in high average scores and percentage ratings for such events.

However, some concerns were repeatedly highlighted in the negative and neutral comments. The most common complaints included overly theoretical content, lack of clarity in examples, and low classroom interaction. Feedback such as "Examples were not clear" and "Found the session confusing" pointed to the need for better structure and explanation. Neutral feedback also suggested that sessions were sometimes too long or lacked depth in examples. Word clouds further revealed that terms like confusing, unclear, and lengthy appeared frequently in less satisfied responses.

# âœ… Key Recommendations for Event Organizers
Enhance Interactivity: Encourage presenters to include real-time activities, polls, or Q&A to boost engagement.

Balance Theory with Practice: Ensure each session includes practical demonstrations or relatable use cases.

Improve Clarity of Examples: Simplify technical content and provide relatable analogies, especially for beginners.

Time Optimization: Avoid overloading sessions â€” aim for compact, well-paced delivery.

Collect Session-Specific Ratings: Use detailed forms that capture feedback per session/event to pinpoint improvements.

Train Speakers: Provide speaker training on delivery, clarity, and participant engagement techniques.

## 1)Additional Insights (Optional if data available):
Top 3 Events with Highest Satisfaction:
FYBA â€“ Average Score: 4.55

MSc Analytical Chemistry Sem I â€“ Average Score: 4.53

TYBSc â€“ Average Score: 4.52

These courses consistently received highly positive feedback from students, indicating strong engagement, effective instruction, or well-organized content.





## 2)Most Common Complaints (Word Cloud): Unclear content, fast pace, low interaction.

 Most Common Complaints (with Frequency Counts):
Based on the negative feedback, the most common complaints identified were:

"Average" â€” mentioned 16 times: Indicates students felt the session was mediocre or lacked impact.

"Satisfied", "Teaching", "Method" â€” each mentioned 13 times: Suggests concerns with the teaching method or that satisfaction was low.

"Expected", "Felt", "Boring" â€” each appeared 9 times: Students often felt bored or the session didnâ€™t meet expectations.

"Confusing", "Session", "Found" â€” each mentioned 5 times: Points to unclear content or a lack of clarity during sessions.

These patterns indicate areas that could be improved, such as making content more engaging, clarifying delivery, and aligning sessions with student expectations.



## 3)Rating vs Event Type (Workshop vs Seminar):

The average satisfaction ratings vary significantly across different event types (represented here by course names). Based on the collected feedback:

ðŸ”¹ MSC Information Technology and Bachelor of Commerce (Banking and Insurance) received the highest average rating of 4.35, indicating high levels of student satisfaction.

ðŸ”¹ Bachelor of Arts also performed well, with an average rating of 4.33.

ðŸ”¸ Bachelor of Management Studies and Bachelor of Commerce (Accounting and Finance) followed closely with average scores of 4.09 and 4.08, respectively.

âš ï¸ Courses like MSC Data Science (3.05), B.Sc. Computer Science (3.35), and B.Voc Food Technology (3.40) had relatively lower satisfaction levels, suggesting possible areas for improvement.

ðŸ§ª Among science streams, MSC Analytical Chemistry (4.03) and MSC Organic Chemistry (3.97) showed moderate satisfaction, while MSC Physics (3.42) and MSC Microbiology (3.54) scored on the lower end.

This variation in ratings can help stakeholders understand which departments or course events are engaging students effectively and which may need enhancements.



## 4)Most-Liked Departments:

ðŸ… Most-Liked Departments
Based on the average satisfaction scores submitted by students, the top-performing departments are:

MSC Information Technology and Bachelor of Commerce (Banking and Insurance) received the highest average rating of 4.35, indicating strong satisfaction levels among students in these courses.

Bachelor of Arts followed closely with an average score of 4.33.

Other well-performing departments include Bachelor of Management Studies (4.09) and Bachelor of Commerce (Accounting and Finance) (4.08).

Departments with moderate satisfaction include MSC Analytical Chemistry (4.03), MSC Organic Chemistry (3.97), and Bachelor of Science/MSC Computer Science (both 3.85).

Lower-rated departments include MSC Data Science with an average rating of 3.05, B.Sc. Computer Science (3.35), and MSC Physics (3.42), suggesting opportunities for improvement.
"""

top_events = (
    df.groupby("Course Name")["Average_Score"]
    .mean()
    .sort_values(ascending=False)
    .head(3)
)

print(top_events)

import matplotlib.pyplot as plt

# Assuming top_events is already defined as you showed
top_events = (
    df.groupby("Course Name")["Average_Score"]
    .mean()
    .sort_values(ascending=False)
    .head(3)
)

# Plotting the bar chart
plt.figure(figsize=(8,5))
bars = plt.bar(top_events.index, top_events.values, color='mediumseagreen')

# Add value labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.03, f'{height:.2f}', ha='center', fontsize=12)

plt.title("Top 3 Events with Highest Average Satisfaction Score")
plt.ylabel("Average Score")
plt.ylim(0, max(top_events.values) + 0.5)
plt.xticks(rotation=30, ha='right')
plt.grid(False)
plt.tight_layout()
plt.show()

"""# ðŸ”¹ 1. Text Vectorization(Not necessary for this projects still i did it for future ML analysis)
To Turn Cleaned_Comments into numerical form I used

CountVectorizer or TfidfVectorizer



"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['Cleaned_Comments'])

from sklearn.feature_extraction.text import TfidfVectorizer

# Step 1: Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=1000)

# Step 2: Fit and transform your cleaned text
X_tfidf = vectorizer.fit_transform(df["Cleaned_Comments"])

# Step 3: Convert to DataFrame (optional, for inspection)
tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())

# Step 4: Check shape
print("TF-IDF Matrix shape:", X_tfidf.shape)

# Step 5: View sample
tfidf_df.head()

"""## TF-IDF (Optional Advanced NLP)

## Logistic Regression

### Build a classification model (e.g. to predict Sentiment from comments)
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Features are the TF-IDF vectors of cleaned comments
X = X_tfidf  # Already created using TF-IDF earlier

# Target is the sentiment label
y = df["Sentiment"]

"""### Train and Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=y)

from sklearn.linear_model import LogisticRegression

# Train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""### Evaluate the model"""

y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

# Confusion Matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

"""### We get 100%accuracy because dataset is clean and there is no imbalance in dataset

##Plot Confusion matrix(seaborn)
"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Define labels
labels = ["Negative", "Neutral", "Positive"]

# Create confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=labels)

# Plot
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Sentiment")
plt.ylabel("Actual Sentiment")
plt.title("Confusion Matrix")
plt.grid(False)  # Remove gridlines
plt.tight_layout()
plt.show()

"""## Chcek for Class Imbalance"""

df['Sentiment'].value_counts(normalize=True) * 100

"""## Chcek For cross Validation"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print("Cross-Validation Accuracy Scores:", scores)
print("Average Accuracy:", scores.mean())

"""## Checking Class Weight To handle imbalance without resampling:


"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

"""## Trying SMOTE or RandomOversampler"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

"""## Evaluate Using F1-Score (Not Just Accuracy)"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, df['Sentiment'], test_size=0.2, stratify=df['Sentiment'], random_state=42
)

# Step 2: Train the model with class_weight
model_rf = RandomForestClassifier(class_weight='balanced', random_state=42)
model_rf.fit(X_train, y_train)

# Step 3: Predict
y_pred_rf = model_rf.predict(X_test)

# Step 4: Evaluation
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))

# Step 5: Confusion Matrix (Visual)
cm = confusion_matrix(y_test, y_pred_rf, labels=["Positive", "Neutral", "Negative"])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Positive", "Neutral", "Negative"],
            yticklabels=["Positive", "Neutral", "Negative"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.grid(False)
plt.show()

"""## Feature Importance Analysis"""

importances = model_rf.feature_importances_
feature_names = vectorizer.get_feature_names_out()
sorted_indices = importances.argsort()[::-1]
for i in sorted_indices[:10]:
    print(f"{feature_names[i]}: {importances[i]:.4f}")

import pandas as pd
import matplotlib.pyplot as plt

# Assuming importances and feature_names are defined as before
top_n = 10
top_features = [(feature_names[i], importances[i]) for i in sorted_indices[:top_n]]

# Create a DataFrame for better visualization
df_features = pd.DataFrame(top_features, columns=["Feature", "Importance"])

print(df_features)

# Optional: Plot feature importances
plt.figure(figsize=(8,6))
plt.barh(df_features["Feature"][::-1], df_features["Importance"][::-1], color='skyblue')
plt.xlabel("Feature Importance")
plt.title("Top 10 Important Features for Sentiment Prediction")
plt.show()

"""## Saving best model"""

import joblib

# Define full path with filename
model_path = r'C:\Users\Gouthum\Downloads\sentiment_rf_model.pkl'

# Save the model
joblib.dump(model_rf, model_path)

import joblib
import os

# Define the folder and filename
folder_path = r'C:\Users\Gouthum\Downloads\Future Interns(Internship)\Task3\Update'
file_name = 'best_model.pkl'
full_path = os.path.join(folder_path, file_name)

# Create folder if it doesn't exist
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

# Save the model
joblib.dump(model_rf, full_path)

# Verify if file exists after saving
if os.path.isfile(full_path):
    print(f"Model saved successfully at: {full_path}")
else:
    print("Failed to save the model.")

import joblib

# Define full path where the model should be saved
model_path = r'C:\Users\Gouthum\Downloads\Future Interns(Internship)\Task3\Update\sentiment_rf_model.pkl'

# Save the model using open() and joblib
with open(model_path, 'wb') as file:
    joblib.dump(model_rf, file)

print("âœ… Model saved successfully at:", model_path)

import joblib

model_path = r'C:\Users\Gouthum\Downloads\Future Interns(Internship)\Task3\Update\sentiment_rf_model.pkl'

# Replace this with your actual trained model
# from sklearn.ensemble import RandomForestClassifier
# model_rf = RandomForestClassifier().fit(X_train, y_train)

# Saving the model
with open(model_path, 'wb') as file:
    joblib.dump(model_rf, file)

print(f"âœ… Model saved at: {model_path}")

from google.colab import files
import joblib

# Save locally inside Colab first
joblib.dump(model_rf, "sentiment_rf_model.pkl")

# Download to your computer
files.download("sentiment_rf_model.pkl")

from google.colab import files
import joblib

# Upload the saved model file
uploaded = files.upload()

# Load the model
model = joblib.load("sentiment_rf_model.pkl")

# Now you can use model.predict() etc.

"""## Deployment

### Save the Vectorizer
"""

import joblib
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

from google.colab import files

# Download the saved file
files.download("tfidf_vectorizer.pkl")

import joblib
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# Load saved model and vectorizer
model = joblib.load("sentiment_rf_model.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")

# Define the cleaning function (same as training)
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    tokens = re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())
    filtered = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(filtered)

# Define prediction function
def predict_sentiment(comment):
    cleaned = clean_text(comment)
    vect = vectorizer.transform([cleaned])
    pred = model.predict(vect)[0]
    return pred

comment = "The session was really informative and enjoyable!"
print("Predicted Sentiment:", predict_sentiment(comment))

comment = "it was confusing."
print("Predicted Sentiment:", predict_sentiment(comment))

comment = "too much thoery.Not my cup of tea"
print("Predicted Sentiment:", predict_sentiment(comment))

"""## Topic Modeling(Advance NLP)"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt

documents = df['Cleaned_Comments'].values

"""## Vectorize text using CountVectorizer"""

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(documents)

dtm

vectorizer

"""## Train LDA model"""

lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_model.fit(dtm)

"""## Display top words per topic"""

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx + 1}:")
        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
        print(", ".join(top_features))
        print()

feature_names = vectorizer.get_feature_names_out()
display_topics(lda_model, feature_names, 10)  # top 10 words per topic

!pip install pyLDAvis

"""## Streamlit App"""

!pip install streamlit

import streamlit as st
import joblib
import numpy as np

# Load saved model and vectorizer
model = joblib.load('sentiment_rf_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Title
st.title("Sentiment Analysis App")

# Input box
user_input = st.text_area("Enter your comment here:")

if st.button("Analyze Sentiment"):
    if user_input.strip():
        # Vectorize input
        input_vect = vectorizer.transform([user_input])

        # Predict sentiment
        prediction = model.predict(input_vect)[0]

        # Show result
        st.write(f"Predicted Sentiment: **{prediction}**")
    else:
        st.write("Please enter some text to analyze.")



"""## Mordern Look"""

code = '''import streamlit as st
import joblib
import numpy as np
import pandas as pd

# Load model and vectorizer
model = joblib.load("sentiment_rf_model.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")

# Streamlit page settings
st.set_page_config(
    page_title="ðŸŽ“ Student Sentiment Analyzer",
    page_icon="ðŸ’¬",
    layout="centered",
    initial_sidebar_state="auto"
)

# Sidebar Info
with st.sidebar:
    st.title("ðŸ“˜ About")
    st.markdown(\"\"\"
    This app analyzes **student feedback** and classifies the sentiment as:

    - ðŸŸ¢ Positive
    - ðŸŸ¡ Neutral
    - ðŸ”´ Negative

    Built using **Streamlit**, **scikit-learn**, and **NLP (TF-IDF + RandomForest)**.
    \"\"\")

# Main Header
st.markdown(
    "<h1 style='text-align: center; color: #4CAF50;'>ðŸ“Š Student Feedback Sentiment Analyzer</h1>",
    unsafe_allow_html=True
)
st.write("")

# Input Box
user_input = st.text_area("âœï¸ Enter Student Comment Below:")

# Predict Button
if st.button("ðŸ” Predict Sentiment"):
    if user_input.strip() == "":
        st.warning("âš ï¸ Please enter a comment before predicting.")
    else:
        input_vector = vectorizer.transform([user_input])
        pred = model.predict(input_vector)[0]

        sentiment_map = {0: "ðŸ”´ Negative", 1: "ðŸŸ¡ Neutral", 2: "ðŸŸ¢ Positive"}
        sentiment_label = sentiment_map.get(pred, str(pred))

        # Result Card
        st.markdown("---")
        st.markdown(f"<h3 style='text-align: center;'>Prediction:</h3>", unsafe_allow_html=True)
        st.markdown(
            f"<h2 style='text-align: center; color: #2E8B57;'>{sentiment_label}</h2>",
            unsafe_allow_html=True
        )
        st.markdown("---")
        st.success("âœ… Sentiment analysis complete.")

# Footer
st.markdown(
    "<hr><div style='text-align: center;'>Made with â¤ï¸ by Gouthum â€¢ Future Interns Task 3</div>",
    unsafe_allow_html=True
)
'''

# Save to a Python file in Colab
with open("app.py", "w") as f:
    f.write(code)

"""## Classic Look"""

code = '''import streamlit as st
import joblib
import numpy as np
import pandas as pd

# Load model and vectorizer
model = joblib.load("sentiment_rf_model.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")

st.title("Student Feedback Sentiment Analysis")

st.write(\"\"\"
Enter a student comment below and see the predicted sentiment.
\"\"\")

# Input text box
user_input = st.text_area("Enter Comment here:")

if st.button("Predict Sentiment"):
    if user_input.strip() == "":
        st.warning("Please enter some text!")
    else:
        # Preprocess & vectorize input
        input_vector = vectorizer.transform([user_input])
        # Predict sentiment
        pred = model.predict(input_vector)[0]

        # Map sentiment labels if needed (e.g., 0->Negative, 1->Neutral, 2->Positive)
        sentiment_map = {0: "Negative", 1: "Neutral", 2: "Positive"}
        sentiment_label = sentiment_map.get(pred, str(pred))

        st.success(f"Predicted Sentiment: **{sentiment_label}**")
'''

# Save to a Python file in Colab first
with open("app.py", "w") as f:
    f.write(code)

from google.colab import files
files.download("app.py")

"""## Run Streamlit App in Anaconda CLI:
ðŸ”¹ 1. Open Anaconda Prompt
Press Win and search for Anaconda Prompt, then open it.

ðŸ”¹ 2. Navigate to your project folder

Paste this command:

cd "C:\Users\Gouthum\Downloads\Future Interns(Internship)\BestModel"
(Make sure the folder path is inside quotes because of the space in folder name.)

ðŸ”¹ 3. (Optional) Create and activate a virtual environment

conda create -n feedback_app python=3.10 -y
conda activate feedback_app


ðŸ”¹ 4. Install required packages

pip install streamlit pandas numpy scikit-learn joblib


ðŸ”¹ 5. Run your Streamlit app

streamlit run app.py
âœ… This will launch a browser window with your Streamlit app.

## Save updated columns
"""

# Save the updated DataFrame to a CSV in Colab
df.to_csv("Final_Student_Feedback_Updated.csv", index=False)

from google.colab import files
files.download("Final_Student_Feedback_Updated.csv")

# Check for leading/trailing spaces in all string columns
trailing_check = df.applymap(lambda x: isinstance(x, str) and (x != x.strip()))

# Show which cells have trailing spaces
has_trailing = trailing_check.any(axis=1)

# Print rows with any trailing/leading spaces
df[has_trailing]

print(f"Rows with trailing/leading spaces: {has_trailing.sum()}")

# Remove leading/trailing spaces from all string columns
df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Recheck for any remaining trailing/leading spaces
trailing_check = df.applymap(lambda x: isinstance(x, str) and (x != x.strip()))
print(f"Rows with trailing/leading spaces after cleaning: {trailing_check.any(axis=1).sum()}")

# Save to CSV
df.to_csv("Cleaned_Student_Satisfaction_Survey.csv", index=False)

# Download to your computer
from google.colab import files
files.download("Cleaned_Student_Satisfaction_Survey.csv")

